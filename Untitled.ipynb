{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=unpickle('data_batch_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[dataset[b'labels']]\n",
    "y=np.transpose(y)\n",
    "y = np.asmatrix(y)\n",
    "\n",
    "num_labels = 10\n",
    "lammbda = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[dataset[b'data']]\n",
    "X=np.transpose(X)\n",
    "X = np.asmatrix(X)\n",
    "X=np.transpose(X)\n",
    "num_row,num_col=X.shape\n",
    "m=num_row #num of training examples\n",
    "\n",
    "input_layer_size=num_col\n",
    "hidden_layer_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParams(w1,w2):\n",
    "    #Get Wl and W2 unrolled into vector:\n",
    "    params = np.concatenate(( np.array(w1.flatten()), np.array(w2.flatten()) ),axis=0)\n",
    "    return params\n",
    "\n",
    "def setParams(params, hiddenLayerSize, inputLayerSize,outputLayerSize):\n",
    "    #Set wi and W2 using single paramater vector.\n",
    "    W1_start = 0\n",
    "    W1_end = hiddenLayerSize * inputLayerSize\n",
    "    W1 = np. reshape (params[W1_start:W1_end], (inputLayerSize , hiddenLayerSize))\n",
    "    W2_end = W1_end + hiddenLayerSize*outputLayerSize\n",
    "    W2 = np.reshape(params[W1_end: W2_end], (hiddenLayerSize, outputLayerSize))\n",
    "    return W1,W2\n",
    "\n",
    "def randinitialiseWeights(L_in,L_out):\n",
    "        \n",
    "        epsilon_init = 44\n",
    "        W =np.random.randint(-1*epsilon_init,epsilon_init,size = ( L_out,1+ L_in))\n",
    "        W=np.divide(W, 1000)\n",
    "     \n",
    "        return W\n",
    "\n",
    "def sigmoid(X):\n",
    "        z = 1.0/(1.0 + np.exp(-X))\n",
    "        return z\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "        g=np.multiply( sigmoid(z), (np.ones(z.shape)-sigmoid(z)) )\n",
    "        return g\n",
    "\n",
    "\n",
    "def costFunction(nn_params, X, y ,num_labels,lammbda,input_layer_size,hidden_layer_size):\n",
    "        global counter\n",
    "\n",
    "        m=y.size\n",
    "\n",
    "        # initial_Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "        #                 (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "        # initial_Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "        #                 (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "\n",
    "        initial_Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "        initial_Theta2 = nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "\n",
    "        J = 0\n",
    "\n",
    "        eX= np.insert(X, 0, 1, axis=1)\n",
    "        eXT=np.transpose(eX)\n",
    "\n",
    "        for i in range(m):\n",
    "                #a1=np.transpose(np.matrix(X[i]))\n",
    "\n",
    "                z2 = np.dot(initial_Theta1,eXT[:,i])\n",
    "\n",
    "                a2=sigmoid(z2)\n",
    "\n",
    "                a2=np.insert(a2, 0, 1, axis=0)\n",
    "\n",
    "                z3=np.dot(initial_Theta2,a2)\n",
    "\n",
    "                hyp=sigmoid(z3)\n",
    "\n",
    "                yt = np.zeros((num_labels,1))\n",
    "\n",
    "                #print(\"printing yt nowwwwwwwwwww\")\n",
    "                #print(yt)\n",
    "                yt[int(y[i].item())-1] = 1\n",
    "\n",
    "                # temp = -1*(yt).*log(h) - (ones(num_labels,1) - (yt)).*log(ones(num_labels,1) - h);\n",
    "\n",
    "                temp = -1*yt\n",
    "                temp = np.multiply(temp,np.log(hyp))\n",
    "\n",
    "                temp = temp - np.multiply((np.ones((num_labels,1)) - yt), np.log(np.ones((num_labels,1))- hyp))\n",
    "\n",
    "                J = J + np.sum(temp)\n",
    "        \n",
    "\n",
    "        J = J/m\n",
    "\n",
    "        # reg = sum(sum(Theta1(:,2:end).*Theta1(:,2:end))) + sum(sum(Theta2(:,2:end).*Theta2(:,2:end)));\n",
    "        #  reg = reg*lambda/m;\n",
    "        #  reg = reg/2;\n",
    "         \n",
    "        #  J = J + reg;\n",
    "\n",
    "        reg = np.sum( np.multiply(initial_Theta1[:,1:], initial_Theta1[:,1:])) + np.sum( np.multiply(initial_Theta2[:,1:] , initial_Theta2[:,1:]))\n",
    "\n",
    "        reg = reg*lammbda/m\n",
    "        reg = reg/2\n",
    "\n",
    "        J = J + reg\n",
    "\n",
    "        capdelta1 = np.zeros(initial_Theta1.shape)\n",
    "        capdelta2 = np.zeros(initial_Theta2.shape)\n",
    "\n",
    "        eX= np.insert(X, 0, 1, axis=1)\n",
    "        eXT=np.transpose(eX)\n",
    "\n",
    "        for i in range(m):\n",
    "                \n",
    "                z2 = np.dot(initial_Theta1,eXT[:,i])\n",
    "\n",
    "                a2=sigmoid(z2)\n",
    "\n",
    "                a2=np.insert(a2, 0, 1, axis=0)\n",
    "\n",
    "                z3=np.dot(initial_Theta2,a2)\n",
    "\n",
    "                hyp=sigmoid(z3)\n",
    "\n",
    "                yt = np.zeros((num_labels,1))\n",
    "\n",
    "                #print(\"printing yt nowwwwwwwwwww\")\n",
    "                #print(yt)\n",
    "                yt[int(y[i].item())-1] = 1\n",
    "\n",
    "                delt3=hyp-yt\n",
    "                delt2=  np.dot(np.transpose(initial_Theta2[:,1:]) ,delt3)\n",
    "\n",
    "                delt2=np.multiply(delt2,sigmoidGradient(z2))\n",
    "\n",
    "                capdelta2=capdelta2+ np.dot(delt3,np.transpose(a2))\n",
    "                capdelta1=capdelta1+ np.dot(delt2,np.transpose(eXT[:,i]))\n",
    "\n",
    "        Theta1_grad =np.multiply(capdelta1,1/m)\n",
    "        Theta2_grad =np.multiply(capdelta2,1/m)\n",
    "\n",
    "        Theta1_grad[:, 1:input_layer_size+1] = Theta1_grad[:, 1:input_layer_size+1] +np.multiply(initial_Theta1[:, 1:input_layer_size+1], (lammbda / m))\n",
    "        Theta2_grad[:, 1:hidden_layer_size+1] = Theta2_grad[:, 1:hidden_layer_size+1] + np.multiply(initial_Theta2[:, 1:hidden_layer_size+1],(lammbda / m))\n",
    "\n",
    "        #grad = np.concatenate(( , np.array(Theta2_grad.flatten()) ), axis=1)\n",
    "        \n",
    "        \n",
    "        return J,Theta1_grad,Theta2_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(nn_params,X,y,lammbda,num_labels,hidden_layer_size,input_layer_size):\n",
    "    \n",
    "        global counter\n",
    "\n",
    "        m=y.size\n",
    "\n",
    "        initial_Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "        initial_Theta2 = nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "\n",
    "        capdelta1 = np.zeros(initial_Theta1.shape)\n",
    "        capdelta2 = np.zeros(initial_Theta2.shape)\n",
    "        \n",
    "        Theta1_grad = np.zeros(initial_Theta1.shape)\n",
    "        Theta2_grad = np.zeros(initial_Theta2.shape)\n",
    "\n",
    "        eX= np.insert(X, 0, 1, axis=1)\n",
    "        eXT=np.transpose(eX)\n",
    "\n",
    "        for i in range(m):\n",
    "                \n",
    "                \n",
    "                \n",
    "                z2 =np.dot(initial_Theta1,eXT[:,i])\n",
    "                \n",
    "                \n",
    "\n",
    "                a2=sigmoid(z2)\n",
    "\n",
    "                a2=np.insert(a2, 0, 1, axis=0)\n",
    "\n",
    "                z3=np.dot(initial_Theta2,a2)\n",
    "\n",
    "                hyp=sigmoid(z3)\n",
    "\n",
    "                yt = np.zeros((num_labels,1))\n",
    "\n",
    "                #print(\"printing yt nowwwwwwwwwww\")\n",
    "                #print(yt)\n",
    "                yt[int(y[i].item())-1] = 1\n",
    "\n",
    "                delt3=hyp-yt\n",
    "                delt2=  np.dot(np.transpose(initial_Theta2[:,1:]) ,delt3)\n",
    "\n",
    "                delt2=np.multiply(delt2,sigmoidGradient(z2))\n",
    "\n",
    "                capdelta2=capdelta2+ np.dot(delt3,np.transpose(a2))\n",
    "                capdelta1=capdelta1+ np.dot(delt2,np.transpose(eXT[:,i]))\n",
    "\n",
    "        Theta1_grad =np.multiply(capdelta1,1/m)\n",
    "        Theta2_grad =np.multiply(capdelta2,1/m)\n",
    "\n",
    "        Theta1_grad[:, 1:input_layer_size+1] = Theta1_grad[:, 1:input_layer_size+1] +np.multiply(initial_Theta1[:, 1:input_layer_size+1], (lammbda / m))\n",
    "        Theta2_grad[:, 1:hidden_layer_size+1] = Theta2_grad[:, 1:hidden_layer_size+1] + np.multiply(initial_Theta2[:, 1:hidden_layer_size+1],(lammbda / m))\n",
    "\n",
    "        #grad = np.concatenate(( , np.array(Theta2_grad.flatten()) ), axis=1)\n",
    "        \n",
    "        \n",
    "        return Theta1_grad,Theta2_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1=randinitialiseWeights(num_col,20)\n",
    "initial_Theta2=randinitialiseWeights(20,10)\n",
    "\n",
    "\n",
    "initial_nn_params = np.concatenate(( np.array(initial_Theta1.flatten()), np.array(initial_Theta2.flatten()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGradientsCheck(X, y,initial_Theta1,initial_Theta2,num_labels,lammbda):\n",
    "    paramsInitial = getParams(initial_Theta1,initial_Theta2)\n",
    "    chkgrad = np.zeros(paramsInitial.shape)\n",
    "    perturb = np.zeros(paramsInitial.shape)\n",
    "    e = 1e-4\n",
    "    hls=15\n",
    "    ils=100\n",
    "    ols=10\n",
    "\n",
    "    backprop_grad1,backprop_grad2=backpropagation(paramsInitial, X, y, lammbda, ols, hls, ils)\n",
    "    backprop_grad = np.concatenate((np.array( backprop_grad1.flatten()),np.array( backprop_grad2.flatten())),axis=1)\n",
    "\n",
    "    for p in range(len(paramsInitial)):\n",
    "        #Set perturbation vector\n",
    "        perturb[p] = e\n",
    "        \n",
    "        initial_Theta1,initial_Theta2= setParams(paramsInitial + perturb,hls,ils,ols)\n",
    "        \n",
    "        loss2,_,_ = costFunction(X, y,initial_Theta1,initial_Theta2,num_labels,lammbda,hls)\n",
    "\n",
    "        initial_Theta1,initial_Theta2= setParams(paramsInitial - perturb,hls,ils,ols)\n",
    "        lossl,_,_ = costFunction(X, y,initial_Theta1,initial_Theta2,num_labels,lammbda,hls)\n",
    "\n",
    "        #Compute Check Gradient\n",
    "        chkgrad[p] = (loss2 - lossl) / (2*e)\n",
    "        #Return the value we changed to zero:\n",
    "        perturb[p] = 0\n",
    "\n",
    "        print(\"BackpropGrad=%d   CheckGrad=%d\",backprop_grad[p],chkgrad[p])\n",
    "    #Return Params to original value\n",
    "    \n",
    "\n",
    "    return chkgrad\n",
    "\n",
    "counter=0\n",
    "\n",
    "def gradientDescentnn(X,y,initial_nn_params,alpha,num_iters,lammbda,input_layer_size, hidden_layer_size, num_labels):\n",
    "    global counter\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    Theta1 = initial_nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "    Theta2 = initial_nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "    \n",
    "    m=len(y)\n",
    "    #J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        nn_params = np.append(np.array( Theta1.flatten()),np.array( Theta2.flatten()))\n",
    "        J,grad1, grad2 = fn.costFunction(nn_params, X, y, num_labels, lammbda, input_layer_size, hidden_layer_size)\n",
    "        Theta1 = Theta1 - (alpha * grad1)\n",
    "        Theta2 = Theta2 - (alpha * grad2)\n",
    "        print(J)\n",
    "        print(\" \")\n",
    "        counter=counter+1\n",
    "        print(counter)\n",
    "    \n",
    "    nn_params = np.concatenate((np.array( Theta1.flatten()),np.array( Theta2.flatten())),axis=1)\n",
    "    return nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debugInitializeWeights(fan_out, fan_in):\n",
    "\n",
    "    w = np.zeros((fan_out, 1 + fan_in))\n",
    "    # we initialise it with sin to ensure it remains same\n",
    "\n",
    "    k = 1\n",
    "    for i in range(fan_out):\n",
    "\n",
    "        for j in range(fan_in+1):\n",
    "\n",
    "            w[i, j] = np.sin(k)\n",
    "            k = k+1\n",
    "    return w\n",
    "\n",
    "def computeNumericalGradient(Theta1,Theta2,X_check,y_check,lammbda,num_labels,input_layer_size,hidden_layer_size):\n",
    "    numgrad1 = np.zeros((Theta1.shape))\n",
    "    numgrad2 = np.zeros((Theta2.shape))\n",
    "    perturb1 = np.zeros((Theta1.shape))\n",
    "    perturb2 = np.zeros((Theta2.shape))\n",
    "    e = 1e-4\n",
    "\n",
    "    for i in range(Theta1.shape[0]):\n",
    "        for j in range(Theta1.shape[1]):\n",
    "\n",
    "            perturb1[i][j]= e\n",
    "            #nn_params, X, y ,num_labels,lammbda,input_layer_size,hidden_layer_size\n",
    "\n",
    "            nn_params1 = np.concatenate(( np.array((Theta1-perturb1).flatten()), np.array((Theta2).flatten()) ))\n",
    "            nn_params2 = np.concatenate(( np.array((Theta1+perturb1).flatten()), np.array((Theta2).flatten()) ))\n",
    "\n",
    "            loss1,_ = fn.costFunction(nn_params1,X_check,y_check,num_labels,lammbda,input_layer_size,hidden_layer_size)\n",
    "            loss2,_ = fn.costFunction(nn_params2,X_check,y_check,num_labels,lammbda,input_layer_size,hidden_layer_size)\n",
    "\n",
    "            numgrad1[i][j] = (loss2 - loss1) / (2*e)\n",
    "            perturb1[i][j] = 0\n",
    "\n",
    "    for i in range(Theta2.shape[0]):\n",
    "        for j in range(Theta2.shape[1]):\n",
    "\n",
    "            perturb2[i][j]= e\n",
    "\n",
    "            nn_params1 = np.concatenate(( np.array((Theta1).flatten()), np.array((Theta2-perturb2).flatten()) ))\n",
    "            nn_params2 = np.concatenate(( np.array((Theta1).flatten()), np.array((Theta2+perturb2).flatten()) ))\n",
    "\n",
    "\n",
    "            loss1,_ = fn.costFunction(nn_params1,X_check,y_check,num_labels,lammbda,input_layer_size,hidden_layer_size)\n",
    "            loss2,_ = fn.costFunction(nn_params2,X_check,y_check,num_labels,lammbda,input_layer_size,hidden_layer_size)\n",
    "            numgrad2[i][j] = (loss2 - loss1) / (2*e)\n",
    "            perturb2[i][j] = 0\n",
    "\n",
    "    return numgrad1, numgrad2\n",
    "    \n",
    "\n",
    "\n",
    "def check_gradients(lammbda):\n",
    "\n",
    "    input_layer_size = 10\n",
    "    hidden_layer_size = 3\n",
    "    num_labels = 10\n",
    "    m = 10\n",
    "\n",
    "    ##generating random test data \n",
    "\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n",
    "\n",
    "    ##generating X using same function\n",
    "\n",
    "    X_check = debugInitializeWeights(m, input_layer_size-1)\n",
    "    #y  = 1 + np.transpose( np.mod(1:m, num_labels) )\n",
    "    y_check = np.zeros((m,1))\n",
    "    for i in range(m):\n",
    "        y_check[i] = ((i+1)%num_labels) +1 \n",
    "\n",
    "    nn_params = np.concatenate(( np.array(Theta1.flatten()), np.array(Theta2.flatten())), axis=0)\n",
    "\n",
    "    #cost = fn.costFunction(nn_params,X_check,y_check,num_labels,lammbda,input_layer_size,hidden_layer_size)\n",
    "    \n",
    "    Xc_ones=np.insert(X_check, 0, 1, axis=1)\n",
    "\n",
    "    #grad1,grad2 = costFunction(nn_params, X_check, y_check, num_labels, lammbda, input_layer_size, hidden_layer_size)\n",
    "    #numgrad = computeNumericalGradient(costFunc, nn_params);\n",
    "    grad1,grad2 = backpropagation(nn_params,X_check,y_check,lammbda,num_labels,hidden_layer_size,input_layer_size)\n",
    "    \n",
    "    numgrad1,numgrad2 = gradientDescentnn(X_checl, y_check, nn_params, 0.8, 200, 0, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "    grad = np.concatenate( ( np.array(grad1.flatten()), np.array(grad2.flatten()) ), axis=1)\n",
    "    \n",
    "    numgrad = nn_params = np.concatenate(( np.array(numgrad1.flatten()), np.array(numgrad2.flatten()) ), axis=0)\n",
    "    # numgrad=[numgrad]\n",
    "    # numgrad=np.array(numgrad)\n",
    "\n",
    "    #diff = np.linalg.norm(numgrad-grad)/ np.linalg.norm(numgrad+grad)\n",
    "\n",
    "    # print(\"if gradients are correct, diff should be less than 1e-9\")\n",
    "    # print(\"diff: \",diff)\n",
    "\n",
    "    #return diff\n",
    "\n",
    "    for i in range(len(numgrad)):\n",
    "        print(\"Numerical Gradient = %f. BackProp Gradient = %f.\"%(grad[i],numgrad[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,10) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a5528997ab28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#nn_params=computeGradientsCheck(X, y, initial_Theta1, initial_Theta2, num_labels, lammbda)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnn_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-c25fc88f66e4>\u001b[0m in \u001b[0;36mcheck_gradients\u001b[1;34m(lammbda)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m#grad1,grad2 = costFunction(nn_params, X_check, y_check, num_labels, lammbda, input_layer_size, hidden_layer_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m#numgrad = computeNumericalGradient(costFunc, nn_params);\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mgrad1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_check\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_check\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlammbda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_layer_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_layer_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mnumgrad1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumgrad2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradientDescentnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_checl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_layer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-ea62bc24cf87>\u001b[0m in \u001b[0;36mbackpropagation\u001b[1;34m(nn_params, X, y, lammbda, num_labels, hidden_layer_size, input_layer_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mdelt2\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_Theta2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mdelt3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mdelt2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelt2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigmoidGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mcapdelta2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcapdelta2\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelt3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,10) (3,) "
     ]
    }
   ],
   "source": [
    "#nn_params=computeGradientsCheck(X, y, initial_Theta1, initial_Theta2, num_labels, lammbda)\n",
    "\n",
    "nn_params=check_gradients(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python383jvsc74a57bd079ca3b7fcb7f4d0b18f483fc131c5a1787f478934ee620a0676c9c3a91447dd2"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}